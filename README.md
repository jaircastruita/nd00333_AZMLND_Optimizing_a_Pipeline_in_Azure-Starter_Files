# Optimizing an ML Pipeline in Azure

## Overview
<br>

This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
<br>

This dataset contains data about direct marketing campaings (phone calls) of a portuguese banking institution. We seek to predict if the client will suscribe a term deposit (variable y).

[Dataset source](https://archive.ics.uci.edu/ml/datasets/bank+marketing)

For this problem the best performing model was a voting ensemble model, with an accuracy of 0.9475, most probably because of the flexibility of its decision boundaries using different participating models in the ensemble.

## Scikit-learn Pipeline
<br>

The pipeline architecture for the dataset transforms categorical features with two categories to binary and with more than two variables replaces the column with an equivalent of dummy variables. The model implemented is a logistic regression with a fixed *l2* regularization and hyperparameters *C* (inverse regularization strength) and *max_iter* (max number of iterations) for hyperparameter tuning.

For the parameter space search a random parameter sampler was used with *C* and *max_iter*, uniform (0.05, 0.1) and choice (100, 150, 200, 250, 300) samplers were selected respectively.
The benefits for these parameter samplers is that makes easier the sampling mechanism to select the hyperparameter's values. In the case of the *C* hyperparameter only the min-max limits have to be chosen, the sampler will explore that space inbetween. In the case of *max_iter*, one specific set of values can be passed and the sampler will randomly select one of them.

Using the bandit policy any model which metric falls above the top 10% will be stopped to not waste resources.

## AutoML
<br>

**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
In contrast one thing that AutoML does in addition to select the most appropiate hyperparameters is selecting different kinds of feature transformations and models. Sometimes there exist a better performing model than the one we have in mind to achieve a better performance metric.

To use an autoML instance some configuration argument are needed.

```python
 automl_config = AutoMLConfig(
    experiment_timeout_minutes=30,
    task='classification',
    primary_metric='AUC_weighted',
    compute_target=compute_target,
    training_data=training_dataset,
    label_column_name="y",
    n_cross_validations=2)
```

Where:
- *experiment_timeout_minutes* determines the max time for the autoML task to complete in minutes.
- *task* is the desired task to be accomplished. It can be 'classification', 'regression' or 'forecast'.
- *primary_metric* is the metric that will be compared in the model / hyperparameter optimization.
- *compute_target* is the compute resources intended to be used to complete the experiment task. If none is given, it is assume that the experiment will be used over local resources.
- *training_data* is the dataset provided to the experiment to fit such models.
- *label_column_name* refers to the target column used for the task to be predicted
- *n_cross_validations* is used to refer the number of folds in order to perform [n cross fold validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)).

## Pipeline comparison
<br>

**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
The performance for the logistic regression and the ensemble model were .9156 and .9475 respectively, giving the lead to the latter. The architecture for both models is in principle different: logistic regression uses a dimension-1 decision hyperplane passed through a logistic function to separate one class versus the other, in contrast the voting ensemble uses several different models with probably different decision function finding mechanisms in order to average them and include them to the ensemble to emit a vote over some observation. some really interesting difference between each one is the intepretability the voting ensemble has to lose in order to (most often than not) increase the evaluation metric.

## Future work
<br>

**What are some areas of improvement for future experiments? Why might these improvements help the model?**
azureml is an in-development library and some models are not implemented yet (catboost for example). Including more models to this experimentation process could find an even better model to achieve better results.

## Proof of cluster clean up
<br>

The proof of cluster deletion is denoted on the output of the last cell on this notebook.

![logs run](images/delete_cluster.PNG)